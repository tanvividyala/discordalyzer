{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6110da68-6e86-4c40-b80a-133d97784487",
   "metadata": {
    "tags": []
   },
   "source": [
    "# conversation analyzer!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8571ab04-8d43-4d32-82e5-f0c67bf60570",
   "metadata": {},
   "source": [
    "What this can do so far: Trends in how many messages sent, trends in word usage. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e57ffc-27c2-4d45-98fe-7aabdeb46102",
   "metadata": {},
   "source": [
    "What I hope to add soon: Text classification, tracking conversation threads, trends in sentiment (positive, negative, and neutral)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf69c475-c7e1-4de7-b47e-30797a9a2094",
   "metadata": {},
   "source": [
    "To use: Replace author1 and author2 with the usernames of the two people in the dm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976bf0d9-42fd-40e1-9882-1fca725664cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65ef0cc7-82ab-4ed6-9cf3-0282032d1879",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "338c6266-abff-4125-ad55-68b276ddfcdb",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m convos \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# insert conversation data csv\u001b[39;00m\n\u001b[1;32m      2\u001b[0m convos \u001b[38;5;241m=\u001b[39m convos\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAttachments\u001b[39m\u001b[38;5;124m'\u001b[39m, axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m convos \u001b[38;5;241m=\u001b[39m convos\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReactions\u001b[39m\u001b[38;5;124m'\u001b[39m, axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1662\u001b[0m     f,\n\u001b[1;32m   1663\u001b[0m     mode,\n\u001b[1;32m   1664\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1665\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1666\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1667\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1668\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1669\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1670\u001b[0m )\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    860\u001b[0m             handle,\n\u001b[1;32m    861\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    862\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    863\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    864\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    865\u001b[0m         )\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: ''"
     ]
    }
   ],
   "source": [
    "convos = pd.read_csv('') # insert conversation data csv\n",
    "convos = convos.drop('Attachments', axis = 1)\n",
    "convos = convos.drop('Reactions', axis = 1)\n",
    "convos = convos.drop('AuthorID', axis = 1)\n",
    "pd.reset_option('all')\n",
    "pd.set_option('display.max_rows', 500) # edit if you'd like"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa6eb4c-47c4-428f-a9ee-971d7e318008",
   "metadata": {
    "tags": []
   },
   "source": [
    "## finding all messages in a date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc22d298-7b4a-494c-8ecc-824782a1fbe4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'convos' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m start_date \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2018-06-13T00:00:00.0000000-00:00\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m end_date \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2024-08-05T00:00:00.0000000-00:00\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 4\u001b[0m filter_convos \u001b[38;5;241m=\u001b[39m convos\u001b[38;5;241m.\u001b[39mloc[(convos\u001b[38;5;241m.\u001b[39mDate \u001b[38;5;241m>\u001b[39m start_date) \u001b[38;5;241m&\u001b[39m (convos\u001b[38;5;241m.\u001b[39mDate \u001b[38;5;241m<\u001b[39m end_date)]\n\u001b[1;32m      5\u001b[0m filter_convos\n\u001b[1;32m      7\u001b[0m new_convos \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m: filter_convos\u001b[38;5;241m.\u001b[39mDate\u001b[38;5;241m.\u001b[39mvalues,\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAuthor\u001b[39m\u001b[38;5;124m'\u001b[39m: filter_convos\u001b[38;5;241m.\u001b[39mAuthor\u001b[38;5;241m.\u001b[39mvalues,\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContent\u001b[39m\u001b[38;5;124m'\u001b[39m: filter_convos\u001b[38;5;241m.\u001b[39mContent\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     11\u001b[0m })\n",
      "\u001b[0;31mNameError\u001b[0m: name 'convos' is not defined"
     ]
    }
   ],
   "source": [
    "start_date = '2018-06-13T00:00:00.0000000-00:00'\n",
    "end_date = '2024-08-05T00:00:00.0000000-00:00'\n",
    "\n",
    "filter_convos = convos.loc[(convos.Date > start_date) & (convos.Date < end_date)]\n",
    "filter_convos\n",
    "\n",
    "new_convos = pd.DataFrame({\n",
    "    'Date': filter_convos.Date.values,\n",
    "    'Author': filter_convos.Author.values,\n",
    "    'Content': filter_convos.Content.values\n",
    "})\n",
    "\n",
    "new_convos.reset_index(drop=True, inplace=True)\n",
    "new_convos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719b65bf-153f-4dba-bd37-7cb3d4fae35c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## count instances of a word in a period of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5997c00d-220a-49b1-9d81-3fcb071c6213",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_date = '2018-06-13T00:00:00.0000000-00:00'\n",
    "end_date = '2024-08-05T00:00:00.0000000-00:00'\n",
    "\n",
    "word_to_count = \"hello\" \n",
    "instances = []\n",
    "count = convos.loc[(convos.Date > start_date) & (convos.Date < end_date)].Content.astype(str).str.contains(word_to_count, case=False, na=False).sum()\n",
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d86a1a7-7661-4951-bf93-a83f6fa62082",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## messages sent over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00aba5e4-ed24-42fe-858c-6cd105f0404d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "convos['Date'] = pd.to_datetime(convos['Date'], utc=True)\n",
    "\n",
    "monthly_totals = convos.groupby(pd.Grouper(key='Date', freq='M')).size() \n",
    "\n",
    "plt.plot(monthly_totals.index, monthly_totals.values)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Proportion')\n",
    "plt.title(f'Messages Sent Over Time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3a1e0a-591c-49f1-919c-4cf64bb0703a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## trends in word/phrase use by person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a29f45-8aba-4386-aaa5-63e0e7755d8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "convos['Date'] = pd.to_datetime(convos['Date'], utc=True)\n",
    "\n",
    "word_to_count = 'word' # input word you want to observe!\n",
    "\n",
    "convos['Keyword_Count'] = convos['Content'].str.contains(word_to_count, case=False)\n",
    "\n",
    "monthly_totals = convos.groupby(pd.Grouper(key='Date', freq='M')).size()\n",
    "monthly_totals = monthly_totals.replace(0, float('nan'))\n",
    "\n",
    "author1_keyword_counts = convos[convos['Author'] == 'author1'].groupby(pd.Grouper(key='Date', freq='M'))['Keyword_Count'].sum()\n",
    "author2_keyword_counts = convos[convos['Author'] == 'author2'].groupby(pd.Grouper(key='Date', freq='M'))['Keyword_Count'].sum()\n",
    "\n",
    "author1_proportions = author1_keyword_counts / monthly_totals\n",
    "author2_proportions = author2_keyword_counts / monthly_totals\n",
    "\n",
    "plt.plot(author1_proportions.index, author1_proportions.values, label='author1')\n",
    "plt.plot(author2_proportions.index, author2_proportions.values, label='author2')\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Proportion')\n",
    "plt.title(f'Monthly Proportion of Messages Containing \"{word_to_count}\" Over Time')\n",
    "plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab49b6c5-e8c2-48d4-a37c-63581ac0123a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## trends in word/phrase use overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0a7f83-a807-4e09-9dc4-36ecf6cb2490",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# word 1\n",
    "\n",
    "convos['Date'] = pd.to_datetime(convos['Date'], utc=True)\n",
    "\n",
    "word_to_count = 'Yippee' \n",
    "\n",
    "convos['Keyword_Count'] = convos['Content'].str.contains(word_to_count, case=False)\n",
    "\n",
    "monthly_totals = convos.groupby(pd.Grouper(key='Date', freq='M')).size()\n",
    "\n",
    "monthly_keyword_counts = convos.groupby(pd.Grouper(key='Date', freq='M'))['Keyword_Count'].sum()\n",
    "\n",
    "monthly_totals = monthly_totals.replace(0, float('nan'))  \n",
    "\n",
    "monthly_proportions = monthly_keyword_counts / monthly_totals\n",
    "\n",
    "plt.plot(monthly_proportions.index, monthly_proportions.values, label = word_to_count)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Proportion')\n",
    "plt.title(f'Monthly Proportion of Messages Containing These Words Over Time')\n",
    "\n",
    "\n",
    "# word 2 \n",
    "\n",
    "convos['Date'] = pd.to_datetime(convos['Date'], utc=True)\n",
    "\n",
    "word_to_count = 'LOL' \n",
    "\n",
    "convos['Keyword_Count'] = convos['Content'].str.contains(word_to_count, case=False)\n",
    "\n",
    "monthly_totals = convos.groupby(pd.Grouper(key='Date', freq='M')).size()\n",
    "\n",
    "monthly_keyword_counts = convos.groupby(pd.Grouper(key='Date', freq='M'))['Keyword_Count'].sum()\n",
    "\n",
    "monthly_totals = monthly_totals.replace(0, float('nan'))  \n",
    "\n",
    "monthly_proportions = monthly_keyword_counts / monthly_totals\n",
    "\n",
    "plt.plot(monthly_proportions.index, monthly_proportions.values, label = word_to_count)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f094e4-b387-48c1-b486-45933b712818",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## proportion of messages sent by author1 vs. author2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cacb44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "convos['Date'] = pd.to_datetime(convos['Date'], utc=True)\n",
    "\n",
    "monthly_totals = convos.groupby(pd.Grouper(key='Date', freq='W')).size()\n",
    "\n",
    "sent_by_author1 = convos[convos['Author'] == 'author1'].groupby(pd.Grouper(key='Date', freq='W')).size()\n",
    "\n",
    "sent_by_author2 = convos[convos['Author'] == 'author2'].groupby(pd.Grouper(key = 'Date', freq = 'W')).size()\n",
    "\n",
    "monthly_totals = monthly_totals.replace(0, float('nan')) \n",
    "\n",
    "author1_proportion = sent_by_author1/monthly_totals\n",
    "\n",
    "author2_proportion = sent_by_author2/monthly_totals\n",
    "\n",
    "plt.plot(author1_proportion.index, author1_proportion.values, label = 'author1') # blue\n",
    "\n",
    "plt.plot(author2_proportion.index, author2_proportion.values, label = 'author2') # orange\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Proportion of Messages')\n",
    "plt.title(f'author1 vs. author2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d234abb6-3449-494d-b189-e88c5afb6428",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "convos = convos.drop('Keyword_Count', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabcdfdc-eef2-459d-8170-4e905aa22612",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## day summaries (WIP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138c631a-2576-4be7-be4b-595bf61ba108",
   "metadata": {},
   "source": [
    "retrieve all messages from one day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d034e752-11e1-4cf1-af81-0722bfa86d2e",
   "metadata": {},
   "source": [
    "(this crashes my computer, but when i get an open ai token it's over for y'all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40a5c52-61db-41d7-ab24-3ff2836fac79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "date = '2022-08-14'\n",
    "start_time = f'{date}T00:00:00.0000000-00:00'\n",
    "end_time = f'{date}T23:59:59.9999999-00:00'\n",
    "\n",
    "filter_convos = convos.loc[(convos.Date > start_time) & (convos.Date < end_time)]\n",
    "filter_convos\n",
    "\n",
    "new_convos = pd.DataFrame({\n",
    "    'Author': filter_convos.Author.values,\n",
    "    'Content': filter_convos.Content.values\n",
    "})\n",
    "\n",
    "new_convos.reset_index(drop=True, inplace=True)\n",
    "new_convos"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2edf55ec-f7d7-4922-a82f-8a7fdb38e32e",
   "metadata": {},
   "source": [
    "from gpt4all import GPT4All\n",
    "model = GPT4All(\"Meta-Llama-3-8B-Instruct.Q4_0.gguf\") \n",
    "with model.chat_session():\n",
    "    print(model.generate(f'This is a day's worth of conversation between author 1 (author1) and and author 2 (author2). Summarize the content of the day in 3-5 paragraphs. Be specific about names of people, places, media, and items talked about in the paragraphs you give. Here is the conversation: {new_convos}', max_tokens=1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acbff10-ce41-4b3f-995d-4e72319679f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## conversation topic clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beb2d3a-c978-4591-88a4-f224ae38ee4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "convos = pd.read_csv(\"\") # insert conversation data csv\n",
    "convos = convos.drop('Attachments', axis = 1)\n",
    "convos = convos.drop('Reactions', axis = 1)\n",
    "convos = convos.drop('AuthorID', axis = 1)\n",
    "\n",
    "start_date = '2022-09-01T00:00:00.0000000-00:00'\n",
    "end_date = '2022-10-01T00:00:00.0000000-00:00'\n",
    "convos = convos.loc[(convos.Date > start_date) & (convos.Date < end_date)]\n",
    "convos = convos.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2df04e-b4c5-4a91-b006-75489160ef14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "custom_stop_words = {'in', 'on', 'at', 'by', 'for', 'with', 'about', 'against', 'between',\n",
    "                     'into', 'through', 'during', 'before', 'after', 'above', 'below',\n",
    "                     'to', 'from', 'up', 'down', 'over', 'under', 'again', 'further',\n",
    "                     'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how',\n",
    "                     'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some',\n",
    "                     'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than',\n",
    "                     'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now',\n",
    "                     'idk', 'bc', 'oh', 'yeah', 'think', 'know', 'one', 'yes', 'no', 'abt', 'im',\n",
    "                     'm', 'really', 'want', 'thing', 'still', 'going', 'yea', 'good', 'mhm', \n",
    "                     'go', 'feel', 'said', 'much', 'say', 'll', 'dont', 'didn', 'real', \n",
    "                     'right', 'ok', 'okay', 'wait', 'true', 'fr', 'thats', 'ig', 'went', 'got', \n",
    "                     'way', 'like', 'maybe', 'wdym', 'hes', 'nice', 'ill', 'wa', 'yo', 'ur', 'youre', \n",
    "                     '1230', 'na', 'gon', \"noo\", \"tbh\", \"90\", \"hm\", \"well\", \"see\", \"might\", \"pls\", \"ive\"\n",
    "} \n",
    "\n",
    "# feel free to edit this list for words you don't want to factor into the clustering.\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    words = word_tokenize(text)\n",
    "    words = [word for word in words if word not in stop_words and word not in custom_stop_words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "texts = convos['Content'].dropna().reset_index(drop=True).apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20238dcc-b24a-4e47-8f42-d2a06ef0f20c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_df=0.85, min_df=5, ngram_range=(1, 3))\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "num_clusters = 10\n",
    "kmeans = KMeans(n_clusters=num_clusters, init='k-means++', n_init=10, random_state=0)\n",
    "kmeans.fit(X)\n",
    "\n",
    "convos['cluster'] = pd.Series(kmeans.labels_, index=texts.index)\n",
    "\n",
    "def get_top_terms_per_cluster(kmeans, vectorizer, num_terms=10):\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
    "    top_terms = {}\n",
    "    for i in range(num_clusters):\n",
    "        top_terms[i] = [terms[ind] for ind in order_centroids[i, :num_terms]]\n",
    "    return top_terms\n",
    "\n",
    "top_terms = get_top_terms_per_cluster(kmeans, vectorizer)\n",
    "\n",
    "for cluster_num, terms in top_terms.items():\n",
    "    print(f\"Cluster {cluster_num}: {', '.join(terms)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d68251-bff6-4dad-8a4c-b1a504dde109",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cluster distribution (need to figure out how to get a more even cluster distribution)\n",
    "cluster_counts = pd.Series(kmeans.labels_).value_counts()\n",
    "print(cluster_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dca353-f3e6-4383-8102-85733505e95c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print columns of text in different clusters\n",
    "\n",
    "cluster = 2 # change based on which cluster you want to look at\n",
    "convos[convos.cluster == cluster]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2f93c9-284b-482f-aa27-abd12ecbe5cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## word cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72805684-23ab-42c4-9079-74f581845e38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "convos = pd.read_csv(\"\")\n",
    "start_date = '2022-08-01T00:00:00.0000000-00:00' # edit start and end date\n",
    "end_date = '2022-09-01T00:00:00.0000000-00:00'\n",
    "convos = convos.loc[(convos.Date > start_date) & (convos.Date < end_date)]\n",
    "\n",
    "text_data = ' '.join(convos['Content'].astype(str))\n",
    "\n",
    "# Define custom stop words\n",
    "custom_stop_words = {'in', 'on', 'at', 'by', 'for', 'with', 'about', 'against', 'between',\n",
    "                     'into', 'through', 'during', 'before', 'after', 'above', 'below',\n",
    "                     'to', 'from', 'up', 'down', 'over', 'under', 'again', 'further',\n",
    "                     'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how',\n",
    "                     'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some',\n",
    "                     'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than',\n",
    "                     'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now',\n",
    "                     'idk', 'bc', 'oh', 'yeah', 'think', 'know', 'one', 'yes', 'no', 'abt', 'im',\n",
    "                     'm', 'really', 'want', 'thing', 'still', 'going', 'yea', 'good', 'mhm', \n",
    "                     'go', 'feel', 'said', 'much', 'say', 'll', 'dont', 'didn', 'real', \n",
    "                     'right', 'ok', 'okay', 'wait', 'true', 'fr', 'thats', 'ig', 'went', 'got', \n",
    "                     'way', 'like', 'maybe', 'wdym', 'hes', 'nice', 'ill', 'wa', 'yo', 'ur', 'youre', \n",
    "                     '1230', 'na', 'time', 'year', 'gon', 'https', 'tenor', 'u', 're', 'nan', 'well', \n",
    "                     'tho'\n",
    "}\n",
    "\n",
    "custom_stop_words = STOPWORDS.union(custom_stop_words)\n",
    "\n",
    "# Generate the word cloud\n",
    "wordcloud = WordCloud(stopwords=custom_stop_words, background_color='white').generate(text_data)\n",
    "\n",
    "# Plot the word cloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
